{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_Embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-becVUr5_Q9"
      },
      "source": [
        "<div align=\"center\">\n",
        "<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\n",
        "Applied ML Â· MLOps Â· Production\n",
        "<br>\n",
        "Join 30K+ developers in learning how to responsibly <a href=\"https://madewithml.com/about/\">deliver value</a> with ML.\n",
        "    <br>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <a target=\"_blank\" href=\"https://newsletter.madewithml.com\"><img src=\"https://img.shields.io/badge/Subscribe-30K-brightgreen\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/MadeWithML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/MadeWithML.svg?style=social&label=Star\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n",
        "    <br>\n",
        "    ðŸ”¥&nbsp; Among the <a href=\"https://github.com/topics/deep-learning\" target=\"_blank\">top ML</a> repositories on GitHub\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTdCMVl9YAXw"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "In this lesson, we will motivate the need for embeddings, which are capable of capturing the contextual, semantic and syntactic meaning in data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuabAj4PYj57"
      },
      "source": [
        "<div align=\"left\">\n",
        "<a target=\"_blank\" href=\"https://madewithml.com/courses/basics/embeddings/\"><img src=\"https://img.shields.io/badge/ðŸ“– Read-blog post-9cf\"></a>&nbsp;\n",
        "<a href=\"https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/12_Embeddings.ipynb\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
        "<a href=\"https://colab.research.google.com/github/GokuMohandas/MadeWithML/blob/main/notebooks/12_Embeddings.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9lh8_YvoR50"
      },
      "source": [
        "# Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtu7IU66obsh"
      },
      "source": [
        "While one-hot encoding allows us to preserve the structural information, it does poses two major disadvantages. \n",
        "\n",
        "- linearly dependent on the number of unique tokens in our vocabulary, which is a problem if we're dealing with a large corpus.\n",
        "- representation for each token does not preserve any relationship with respect to other tokens.\n",
        "\n",
        "In this notebook, we're going to motivate the need for embeddings and how they address all the shortcomings of one-hot encoding. The main idea of embeddings is to have fixed length representations for the tokens in a text regardless of the number of tokens in the vocabulary. With one-hot encoding, each token is represented by an array of size [1 X `vocab_size`], but with embeddings, each token now has the shape [1 X `embed_dim`]. The values in the representation will are not fixed binary values but rather, changing floating points allowing for fine-grained learned representations.\n",
        "\n",
        "* **Objective:**  Represent tokens in text that capture the intrinsic semantic relationships.\n",
        "* **Advantages:** \n",
        "    * Low-dimensionality while capturing relationships.\n",
        "    * Interpretable token representations\n",
        "* **Disadvantages:** Can be computationally intensive to precompute.\n",
        "* **Miscellaneous:** There are lot's of pretrained embeddings to choose from but you can also train your own from scratch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH_O4MZ294jk"
      },
      "source": [
        "# Learning Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F47IiPgUupAk"
      },
      "source": [
        "We can learn embeddings by creating our models in PyTorch but first, we're going to use a library that specializes in embeddings and topic modeling called [Gensim](https://radimrehurek.com/gensim/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pZljlaCgG6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45dd7ac-6c70-4f94-ad0e-1db882defea4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt');\n",
        "import numpy as np\n",
        "import re\n",
        "import urllib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oektJd55gG1p"
      },
      "source": [
        "SEED = 1234"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqbnugiD-SW0"
      },
      "source": [
        "# Set seed for reproducibility\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF5D_nNjlx2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f2bd25-d0eb-4f50-feab-263d5ccb81f3"
      },
      "source": [
        "# Split text into sentences\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "book = urllib.request.urlopen(url=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/harrypotter.txt\")\n",
        "sentences = tokenizer.tokenize(str(book.read()))\n",
        "print (f\"{len(sentences)} sentences\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12443 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWyxaKfJOomF"
      },
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"Conditional preprocessing on our text.\"\"\"\n",
        "    # Lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Spacing and filters\n",
        "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
        "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric chars\n",
        "    text = re.sub(' +', ' ', text)  # remove multiple spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Separate into word tokens\n",
        "    text = text.split(\" \")\n",
        "\n",
        "    return text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsZz5jfMlx0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef24bfb9-3f3b-462d-bdd9-d4c525ebd0a5"
      },
      "source": [
        "# Preprocess sentences\n",
        "print (sentences[11])\n",
        "sentences = [preprocess(sentence) for sentence in sentences]\n",
        "print (sentences[11])\n",
        "print (sentences[0:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Snape nodded, but did not elaborate.\n",
            "['snape', 'nodded', 'but', 'did', 'not', 'elaborate']\n",
            "[['b', 'harry', 'potter', 'and', 'the', 'deathly', 'hallows', 'by', 'j', 'k', 'rowling', 'r', 'n', 'r', 'nchapter', 'one', 'the', 'dark', 'lord', 'ascending', 'r', 'nthe', 'two', 'men', 'appeared', 'out', 'of', 'nowhere', 'a', 'few', 'yards', 'apart', 'in', 'the', 'narrow', 'moonlit', 'lane'], ['for', 'a', 'second', 'they', 'stood', 'quite', 'still', 'wands', 'directed', 'at', 'each', 'other', 's', 'chests', 'then', 'recognizing', 'each', 'other', 'they', 'stowed', 'their', 'wands', 'beneath', 'their', 'cloaks', 'and', 'started', 'walking', 'briskly', 'in', 'the', 'same', 'direction'], ['news']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No6c943C-P7o"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeszvcMOji4u"
      },
      "source": [
        "When we have large vocabularies to learn embeddings for, things can get complex very quickly. Recall that the backpropagation with softmax updates both the correct and incorrect class weights. This becomes a massive computation for every backwas pass we do so a workaround is to use [negative sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) which only updates the correct class and a few arbitrary incorrect classes (`NEGATIVE_SAMPLING`=20). We're able to do this because of the large amount of training data where we'll see the same word as the target class multiple times.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rozFTf06ji1b"
      },
      "source": [
        "But how do we learn the embeddings the first place? The intuition behind embeddings is that the definition of a token depends on the token itself but on its context. There are several different ways of doing this:\n",
        "\n",
        "1. Given the word in the context, predict the target word (CBOW - continuous bag of words).\n",
        "2. Given the target word, predict the context word (skip-gram).\n",
        "3. Given a sequence of words, predict the next word (LM - language modeling).\n",
        "\n",
        "All of these approaches involve create data to train our model on. Every word in a sentence becomes the target word and the context words are determines by a window. In the image below (skip-gram), the window size is 2 (2 words to the left and right of the target word). We repeat this for every sentence in our corpus and this results in our training data for the unsupervised task. This in an unsupervised learning technique since we don't have official labels for contexts. The idea is that similar target words will appear with similar contexts and we can learn this relationship by repeatedly training our mode with (context, target) pairs.\n",
        "\n",
        "<div align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/basics/python/skipgram.png\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "We can learn embeddings using any of these approaches above and some work better than others. You can inspect the learned embeddings but the best way to choose an approach is to empirically validate the performance on a supervised task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqKCr--k-f9e"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufU-9l_W-QKj"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "WINDOW = 5\n",
        "MIN_COUNT = 3 # Ignores all words with total frequency lower than this\n",
        "SKIP_GRAM = 1 # 0 = CBOW\n",
        "NEGATIVE_SAMPLING = 20"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha3I2oSsmhJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0648c37-31de-41d1-a866-9fc4f2422c19"
      },
      "source": [
        "# Super fast because of optimized C code under the hood\n",
        "w2v = Word2Vec(\n",
        "    sentences=sentences, size=EMBEDDING_DIM, \n",
        "    window=WINDOW, min_count=MIN_COUNT, \n",
        "    sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)\n",
        "print (w2v)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=4937, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl6oJv8jmhHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6327e823-2920-43d3-f3c0-ec24b6c7c20f"
      },
      "source": [
        "# Vector for each word\n",
        "w2v.wv.get_vector(\"potter\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.28462741e-01,  1.86221108e-01, -4.50927168e-02,  1.77882388e-01,\n",
              "       -3.16583425e-01,  1.47528067e-01, -1.79868504e-01,  1.39167085e-01,\n",
              "       -3.62898111e-01, -4.59536046e-01, -2.31975749e-01, -7.77394325e-02,\n",
              "       -4.08502698e-01, -5.51960692e-02, -2.87412014e-02, -5.81815317e-02,\n",
              "       -1.49024859e-01,  3.55063856e-01, -3.37540358e-02,  2.84497052e-01,\n",
              "        5.69262765e-02,  8.99222493e-02,  2.82944441e-01, -3.31928730e-01,\n",
              "        7.28802830e-02, -3.29693109e-01, -1.37665570e-01, -2.20354974e-01,\n",
              "       -1.17584124e-01,  5.45191988e-02,  4.28497158e-02,  1.72679782e-01,\n",
              "        2.28713527e-01, -8.28511491e-02, -6.01662770e-02, -2.40195140e-01,\n",
              "       -4.44697142e-01,  3.69372852e-02, -5.24608754e-02,  5.43071069e-02,\n",
              "        2.08356917e-01, -2.84519745e-04, -5.67499623e-02, -5.23230851e-01,\n",
              "        3.00690800e-01, -3.52704018e-01, -5.97953975e-01,  1.85622424e-01,\n",
              "        2.15782180e-01, -4.85769249e-02, -2.28079900e-01,  1.48887455e-01,\n",
              "       -1.17754385e-01,  2.34266713e-01,  1.74556166e-01,  3.23159724e-01,\n",
              "        5.76591074e-01, -2.07104310e-01,  2.33430475e-01, -1.72867626e-01,\n",
              "       -1.82802051e-01,  3.50859463e-01, -1.48157716e-01,  1.62250116e-01,\n",
              "       -2.45722204e-01, -4.56982285e-01, -5.22980653e-02,  3.44288319e-01,\n",
              "       -1.35462070e-02,  7.12883994e-02,  9.75091383e-03,  1.09979287e-01,\n",
              "        7.86115006e-02, -2.12158933e-02, -9.88116115e-02,  2.01063529e-02,\n",
              "        4.96916547e-02, -9.25059840e-02,  3.27877730e-01, -2.15915695e-01,\n",
              "        1.28805593e-01, -2.96491504e-01, -1.39901459e-01, -4.15016621e-01,\n",
              "       -8.21251571e-02, -6.49803996e-01,  1.01862453e-01, -5.53404503e-02,\n",
              "        3.43786716e-01, -3.73023003e-01,  6.69919252e-02, -2.30820626e-01,\n",
              "        3.77385288e-01, -2.99350470e-01, -1.35296047e-01, -3.46675247e-01,\n",
              "        9.57322046e-02, -3.66166756e-02, -1.80579394e-01,  6.02967083e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyuLX9DTnLvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5e6ca1-27fe-486c-dc0d-f63f51eb0913"
      },
      "source": [
        "# Get nearest neighbors (excluding itself)\n",
        "w2v.wv.most_similar(positive=\"scar\", topn=5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('forehead', 0.9240695834159851),\n",
              " ('pain', 0.917058527469635),\n",
              " ('prickling', 0.9034652709960938),\n",
              " ('eyes', 0.9031689763069153),\n",
              " ('mouth', 0.9029277563095093)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT7B0KRVTFew"
      },
      "source": [
        "# Saving and loading\n",
        "w2v.wv.save_word2vec_format('model.bin', binary=True)\n",
        "w2v = KeyedVectors.load_word2vec_format('model.bin', binary=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZXVP5vfuiD5"
      },
      "source": [
        "## FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvuoeWYMuqsa"
      },
      "source": [
        "What happen's when a word doesn't exist in our vocabulary? We could assign an UNK token which is used for all OOV (out of vocabulary) words or we could use [FastText](https://radimrehurek.com/gensim/models/fasttext.html), which uses character-level n-grams to embed a word. This helps embed rare words, misspelled words, and also words that don't exist in our corpus but are similar to words in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVg3PBeD-kAa"
      },
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTNW4Mfgrpo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4d3dae-6c0e-4534-d95e-3f167079961e"
      },
      "source": [
        "# Super fast because of optimized C code under the hood\n",
        "ft = FastText(sentences=sentences, size=EMBEDDING_DIM, \n",
        "              window=WINDOW, min_count=MIN_COUNT, \n",
        "              sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)\n",
        "print (ft)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FastText(vocab=4937, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbA4vU5uxiw3"
      },
      "source": [
        "# This word doesn't exist so the word2vec model will error out\n",
        "# w2v.wv.most_similar(positive=\"scarring\", topn=5)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRG30aE4sMjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbca1ef-0d9a-4aee-a0f0-f76938b313e1"
      },
      "source": [
        "# FastText will use n-grams to embed an OOV word\n",
        "ft.wv.most_similar(positive=\"scarring\", topn=5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('trembling', 0.9741309285163879),\n",
              " ('sparkling', 0.9722764492034912),\n",
              " ('shivering', 0.971108078956604),\n",
              " ('clearing', 0.9699482321739197),\n",
              " ('lightning', 0.9698289632797241)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SE5fPMUnLyP"
      },
      "source": [
        "# Save and loading\n",
        "ft.wv.save('model.bin')\n",
        "ft = KeyedVectors.load('model.bin')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67UmjtK0pF9X"
      },
      "source": [
        "# Pretrained embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm1GPn4spF6x"
      },
      "source": [
        "We can learn embeddings from scratch using one of the approaches above but we can also leverage pretrained embeddings that have been trained on millions of documents. Popular ones include [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec) (skip-gram) or [GloVe](https://nlp.stanford.edu/projects/glove/) (global word-word co-occurrence). We can validate that these embeddings captured meaningful semantic relationships by confirming them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh42Mb4lLbuB"
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9gxHJA9M8hK"
      },
      "source": [
        "# Arguments\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANfQHxGrMKTe"
      },
      "source": [
        "def plot_embeddings(words, embeddings, pca_results):\n",
        "    for word in words:\n",
        "        index = embeddings.index2word.index(word)\n",
        "        plt.scatter(pca_results[index, 0], pca_results[index, 1])\n",
        "        plt.annotate(word, xy=(pca_results[index, 0], pca_results[index, 1]))\n",
        "    plt.show()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW9Qtkz3LfdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7a4d42-d2a5-4236-ecaa-ad4bca01c1c3"
      },
      "source": [
        "# Unzip the file (may take ~3-5 minutes)\n",
        "resp = urlopen('http://nlp.stanford.edu/data/glove.6B.zip')\n",
        "zipfile = ZipFile(BytesIO(resp.read()))\n",
        "zipfile.namelist()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['glove.6B.50d.txt',\n",
              " 'glove.6B.100d.txt',\n",
              " 'glove.6B.200d.txt',\n",
              " 'glove.6B.300d.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWnVBrOaLjIC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03b9b7ea-c7ce-4fbb-d9ca-966c93ad213c"
      },
      "source": [
        "# Write embeddings to file\n",
        "embeddings_file = 'glove.6B.{0}d.txt'.format(EMBEDDING_DIM)\n",
        "zipfile.extract(embeddings_file)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/glove.6B.100d.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFLyIqIxrUIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b5190a-10c1-4772-9ec9-ab8cb41fa58c"
      },
      "source": [
        "# Preview of the GloVe embeddings file\n",
        "with open(embeddings_file, 'r') as fp:\n",
        "    line = next(fp)\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embedding = np.asarray(values[1:], dtype='float32')\n",
        "    print (f\"word: {word}\")\n",
        "    print (f\"embedding:\\n{embedding}\")\n",
        "    print (f\"embedding dim: {len(embedding)}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word: the\n",
            "embedding:\n",
            "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
            "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
            " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
            " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
            " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
            "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
            "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
            " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
            "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
            "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
            "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
            " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
            " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
            " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
            "  0.8278    0.27062 ]\n",
            "embedding dim: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eD5doqFLjFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102df9cc-4b4d-4e39-c577-6d174189bae6"
      },
      "source": [
        "# Save GloVe embeddings to local directory in word2vec format\n",
        "word2vec_output_file = '{0}.word2vec'.format(embeddings_file)\n",
        "glove2word2vec(embeddings_file, word2vec_output_file)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To4sx_1iMCX0"
      },
      "source": [
        "# Load embeddings (may take a minute)\n",
        "glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEhBhvgHMEH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb3d6d0-0874-4b12-f9ca-5e18e101f442"
      },
      "source": [
        "# (king - man) + woman = ?\n",
        "glove.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7698541283607483),\n",
              " ('monarch', 0.6843380928039551),\n",
              " ('throne', 0.6755735874176025),\n",
              " ('daughter', 0.6594556570053101),\n",
              " ('princess', 0.6520534753799438)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR94AICkMEFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd51f37-ecd8-49f2-c04d-b8ddfb88bb1a"
      },
      "source": [
        "# Get nearest neighbors (exlcusing itself)\n",
        "glove.wv.most_similar(positive=\"goku\", topn=5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gohan', 0.7246542572975159),\n",
              " ('bulma', 0.6497020125389099),\n",
              " ('raistlin', 0.6443604230880737),\n",
              " ('skaar', 0.6316742897033691),\n",
              " ('guybrush', 0.6231324672698975)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gseqjBmzMECq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbaa789-9c9f-4a7a-83aa-b35e38140739"
      },
      "source": [
        "# Reduce dimensionality for plotting\n",
        "X = glove[glove.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "pca_results = pca.fit_transform(X)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFQWGyncMHgK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "7d99e5b1-19b0-4272-bc9b-0e59115a6085"
      },
      "source": [
        "# Visualize\n",
        "plot_embeddings(\n",
        "    words=[\"king\", \"queen\", \"man\", \"woman\"], embeddings=glove,  \n",
        "    pca_results=pca_results)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzrZ2_RBMHdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7752fadd-fb7b-455a-daf3-4d53f34fc98c"
      },
      "source": [
        "# Bias in embeddings\n",
        "glove.most_similar(positive=['woman', 'doctor'], negative=['man'], topn=5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nurse', 0.7735227346420288),\n",
              " ('physician', 0.7189429998397827),\n",
              " ('doctors', 0.6824328303337097),\n",
              " ('patient', 0.6750682592391968),\n",
              " ('dentist', 0.6726033687591553)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE1kCvwnkBPc"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_DIRj8G5uOC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tdPACZf5uTo"
      },
      "source": [
        "SEED = 1234"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMt7hJuB5uXN"
      },
      "source": [
        "def set_seeds(seed=1234):\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # multi-GPU# Set seeds for reproducibility\n",
        "set_seeds(seed=SEED)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQzlWknv5ua6"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "set_seeds(seed=SEED)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1-vnM-P8i_P",
        "outputId": "4ed5830d-e719-4fbf-827b-c5663511582c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set device\n",
        "cuda = True\n",
        "device = torch.device('cuda' if (\n",
        "    torch.cuda.is_available() and cuda) else 'cpu')\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "if device.type == 'cuda':\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "print (device)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfNfv1NTkKXa"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiQk-yClkL3s"
      },
      "source": [
        "We will download the [AG News dataset](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html), which consists of 120K text samples from 4 unique classes (`Business`, `Sci/Tech`, `Sports`, `World`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HLyMQBDj__P"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNfmmNBokAHI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "90502ac8-89c1-432e-ea95-df90fc8dcf9a"
      },
      "source": [
        "# Load data\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/news.csv\"\n",
        "df = pd.read_csv(url, header=0) # load\n",
        "df = df.sample(frac=1).reset_index(drop=True) # shuffle\n",
        "df.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sharon Accepts Plan to Reduce Gaza Army Operat...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Internet Key Battleground in Wildlife Crime Fight</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>July Durable Good Orders Rise 1.7 Percent</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Growing Signs of a Slowing on Wall Street</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The New Faces of Reality TV</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  category\n",
              "0  Sharon Accepts Plan to Reduce Gaza Army Operat...     World\n",
              "1  Internet Key Battleground in Wildlife Crime Fight  Sci/Tech\n",
              "2          July Durable Good Orders Rise 1.7 Percent  Business\n",
              "3          Growing Signs of a Slowing on Wall Street  Business\n",
              "4                        The New Faces of Reality TV     World"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk0a7TE2kTq4"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeTyLL8-kU9F"
      },
      "source": [
        "We're going to clean up our input data first by doing operations such as lower text, removing stop (filler) words, filters using regular expressions, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIrwF49UkAJ9"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQR8I3HxkAMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a1e002-be96-47e7-b6e5-a234361db1fc"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "STOPWORDS = stopwords.words('english')\n",
        "print (STOPWORDS[:5])\n",
        "porter = PorterStemmer()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g43E1Oa1kAO4"
      },
      "source": [
        "def preprocess(text, stopwords=STOPWORDS):\n",
        "    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n",
        "    # Lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "\n",
        "    # Remove words in paranthesis\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "\n",
        "    # Spacing and filters\n",
        "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
        "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric chars\n",
        "    text = re.sub(' +', ' ', text)  # remove multiple spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsWX-VNQkARQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "923a7d13-49d3-4d8b-ce68-57547d182dd6"
      },
      "source": [
        "# Sample\n",
        "text = \"Great week for the NYSE!\"\n",
        "preprocess(text=text)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'great week nyse'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Up0hTP8kwJx",
        "outputId": "b7b838d8-0222-4149-e41b-ae93817618fa"
      },
      "source": [
        "# Apply to dataframe\n",
        "preprocessed_df = df.copy()\n",
        "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\n",
        "print (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n",
            "\n",
            "sharon accepts plan reduce gaza army operation haaretz says\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QniPnUJYUCiL",
        "outputId": "c03973dc-5a96-4f57-9ac2-1a33e6595ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "preprocessed_df"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sharon accepts plan reduce gaza army operation...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>internet key battleground wildlife crime fight</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>july durable good orders rise 1 7 percent</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>growing signs slowing wall street</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>new faces reality tv</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119995</th>\n",
              "      <td>bush blair see hope palestinian state</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119996</th>\n",
              "      <td>ex soldiers vow bring order haiti capital</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119997</th>\n",
              "      <td>musharraf says u must address root terrorism</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119998</th>\n",
              "      <td>nuclear materials 39 vanish 39 iraq</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119999</th>\n",
              "      <td>brief bowstreet unveils pre packaged portal of...</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    title  category\n",
              "0       sharon accepts plan reduce gaza army operation...     World\n",
              "1          internet key battleground wildlife crime fight  Sci/Tech\n",
              "2               july durable good orders rise 1 7 percent  Business\n",
              "3                       growing signs slowing wall street  Business\n",
              "4                                    new faces reality tv     World\n",
              "...                                                   ...       ...\n",
              "119995              bush blair see hope palestinian state     World\n",
              "119996          ex soldiers vow bring order haiti capital     World\n",
              "119997       musharraf says u must address root terrorism     World\n",
              "119998                nuclear materials 39 vanish 39 iraq     World\n",
              "119999  brief bowstreet unveils pre packaged portal of...  Sci/Tech\n",
              "\n",
              "[120000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F_TNV8Nk7Za"
      },
      "source": [
        "> If you have preprocessing steps like standardization, etc. that are calculated, you need to separate the training and test set first before applying those operations. This is because we cannot apply any knowledge gained from the test set accidentally (data leak) during preprocessing/training. However for global preprocessing steps like the function above where we aren't learning anything from the data itself, we can perform before splitting the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZaX1WFyk8gR"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iy_Ej7ukwMt"
      },
      "source": [
        "import collections\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw5zFpt3k-wz"
      },
      "source": [
        "TRAIN_SIZE = 0.7\n",
        "VAL_SIZE = 0.15\n",
        "TEST_SIZE = 0.15"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHutQd7Pk-zt"
      },
      "source": [
        "def train_val_test_split(X, y, train_size):\n",
        "    \"\"\"Split dataset into data splits.\"\"\"\n",
        "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOz9QSm5lBH7"
      },
      "source": [
        "# Data\n",
        "X = preprocessed_df[\"title\"].values\n",
        "y = preprocessed_df[\"category\"].values"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_ZnJC0AURTl",
        "outputId": "74f34e8e-c8e8-4e7e-e7d7-09eaa1b4cc67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(120000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU8ubwislBKk",
        "outputId": "30da67bc-de4f-4316-b4d4-243ea47536e7"
      },
      "source": [
        "# Create data splits\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
        "    X=X, y=y, train_size=TRAIN_SIZE)\n",
        "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "print (f\"Sample point: {X_train[0]} â†’ {y_train[0]}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (84000,), y_train: (84000,)\n",
            "X_val: (18000,), y_val: (18000,)\n",
            "X_test: (18000,), y_test: (18000,)\n",
            "Sample point: china battles north korea nuclear talks â†’ World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZhaxH8xmHAy"
      },
      "source": [
        "## LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYueMyIUmHEh"
      },
      "source": [
        "Next we'll define a `LabelEncoder` to encode our text labels into unique indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsPgVemMmHJK"
      },
      "source": [
        "import itertools"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlZ4w8OfmHM2"
      },
      "source": [
        "class LabelEncoder(object):\n",
        "    \"\"\"Label encoder for tag labels.\"\"\"\n",
        "    def __init__(self, class_to_index={}):\n",
        "        self.class_to_index = class_to_index\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
        "        self.classes = list(self.class_to_index.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.class_to_index)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
        "\n",
        "    def fit(self, y):\n",
        "        classes = np.unique(y)\n",
        "        for i, class_ in enumerate(classes):\n",
        "            self.class_to_index[class_] = i\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
        "        self.classes = list(self.class_to_index.keys())\n",
        "        return self\n",
        "\n",
        "    def encode(self, y):\n",
        "        encoded = np.zeros((len(y)), dtype=int)\n",
        "        for i, item in enumerate(y):\n",
        "            encoded[i] = self.class_to_index[item]\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, y):\n",
        "        classes = []\n",
        "        for i, item in enumerate(y):\n",
        "            classes.append(self.index_to_class[item])\n",
        "        return classes\n",
        "\n",
        "    def save(self, fp):\n",
        "        with open(fp, 'w') as fp:\n",
        "            contents = {'class_to_index': self.class_to_index}\n",
        "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, fp):\n",
        "        with open(fp, 'r') as fp:\n",
        "            kwargs = json.load(fp=fp)\n",
        "        return cls(**kwargs)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA4TyhuBmHPu",
        "outputId": "883a8643-5e87-4c07-be14-a90a53c7050b"
      },
      "source": [
        "# Encode\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "NUM_CLASSES = len(label_encoder)\n",
        "label_encoder.class_to_index"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIHkazhiUrYD",
        "outputId": "3a75b09d-f801-4303-e387-ef5c0644c2d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['World', 'Sports', 'Sports', ..., 'Business', 'Sports', 'Sports'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjEqwySTmHSg",
        "outputId": "0d7b121f-1413-4d48-bfda-9958470fd21f"
      },
      "source": [
        "# Convert labels to tokens\n",
        "print (f\"y_train[0]: {y_train[0]}\")\n",
        "y_train = label_encoder.encode(y_train)\n",
        "y_val = label_encoder.encode(y_val)\n",
        "y_test = label_encoder.encode(y_test)\n",
        "print (f\"y_train[0]: {y_train[0]}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train[0]: World\n",
            "y_train[0]: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE5-9S6VmHVO",
        "outputId": "83b6f421-586c-45a3-fdca-02252dcf2d60"
      },
      "source": [
        "# Class weights\n",
        "counts = np.bincount(y_train)\n",
        "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
        "print (f\"counts: {counts}\\nweights: {class_weights}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "counts: [21000 21000 21000 21000]\n",
            "weights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TlMos7IVNS-",
        "outputId": "fe3469c2-06ad-4d6c-cccd-f6708f6869b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'china battles north korea nuclear talks'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHRC9EfzlmP-"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkQmzdUXlmUH"
      },
      "source": [
        "We'll define a `Tokenizer` to convert our text input data into token indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcqOl3Lbk-2Q"
      },
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from more_itertools import take"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXY8Hn0gY80D"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbyIehIDl7l-"
      },
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, char_level, num_tokens=None, \n",
        "                 pad_token='<PAD>', oov_token='<UNK>',\n",
        "                 token_to_index=None):\n",
        "        self.char_level = char_level\n",
        "        self.separator = '' if self.char_level else ' '\n",
        "        if num_tokens: num_tokens -= 2 # pad + unk tokens\n",
        "        self.num_tokens = num_tokens\n",
        "        self.pad_token = pad_token\n",
        "        self.oov_token = oov_token\n",
        "        if not token_to_index:\n",
        "            token_to_index = {pad_token: 0, oov_token: 1}\n",
        "        self.token_to_index = token_to_index\n",
        "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_index)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<Tokenizer(num_tokens={len(self)})>\"\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        if not self.char_level:\n",
        "            texts = [text.split(\" \") for text in texts]\n",
        "        all_tokens = [token for text in texts for token in text]\n",
        "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
        "        self.min_token_freq = counts[-1][1]\n",
        "        for token, count in counts:\n",
        "            index = len(self)\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "        return self\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            if not self.char_level:\n",
        "                text = text.split(' ')\n",
        "            sequence = []\n",
        "            for token in text:\n",
        "                sequence.append(self.token_to_index.get(\n",
        "                    token, self.token_to_index[self.oov_token]))\n",
        "            sequences.append(np.asarray(sequence))\n",
        "        return sequences\n",
        "\n",
        "    def sequences_to_texts(self, sequences):\n",
        "        texts = []\n",
        "        for sequence in sequences:\n",
        "            text = []\n",
        "            for index in sequence:\n",
        "                text.append(self.index_to_token.get(index, self.oov_token))\n",
        "            texts.append(self.separator.join([token for token in text]))\n",
        "        return texts\n",
        "\n",
        "    def save(self, fp):\n",
        "        with open(fp, 'w') as fp:\n",
        "            contents = {\n",
        "                'char_level': self.char_level,\n",
        "                'oov_token': self.oov_token,\n",
        "                'token_to_index': self.token_to_index\n",
        "            }\n",
        "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, fp):\n",
        "        with open(fp, 'r') as fp:\n",
        "            kwargs = json.load(fp=fp)\n",
        "        return cls(**kwargs)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZgQb7cdqD6q"
      },
      "source": [
        "> It's important that we only fit using our train data split because during inference, our model will not always know every token so it's important to replicate that scenario with our validation and test splits as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlMH93AKl7oc",
        "outputId": "201a8941-daea-43d5-8ae3-2f97e5e970b9"
      },
      "source": [
        "# Tokenize\n",
        "tokenizer = Tokenizer(char_level=False, num_tokens=5000)\n",
        "tokenizer.fit_on_texts(texts=X_train)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "print (tokenizer)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Tokenizer(num_tokens=5000)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT93_ZFIl7rE",
        "outputId": "be86c71d-bb41-43be-b74d-ff9d08111b80"
      },
      "source": [
        "# Sample of tokens\n",
        "print (take(5, tokenizer.token_to_index.items()))\n",
        "print (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('<PAD>', 0), ('<UNK>', 1), ('39', 2), ('b', 3), ('gt', 4)]\n",
            "least freq token's freq: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XucE1o-JZUm_",
        "outputId": "ae8b42b8-3ae8-4a4a-e6a3-74b6d5a95021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'china battles north korea nuclear talks'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz2XAlijl7u0",
        "outputId": "1e7e88cf-f21e-4443-eb69-2352c42d93a1"
      },
      "source": [
        "# Convert texts to sequences of indices\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
        "print (\"Text to indices:\\n\"\n",
        "    f\"  (preprocessed) â†’ {preprocessed_text}\\n\"\n",
        "    f\"  (tokenized) â†’ {X_train[0]}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to indices:\n",
            "  (preprocessed) â†’ china battles north korea nuclear talks\n",
            "  (tokenized) â†’ [  16 1491  285  142  114   24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "581nl9EYFAsS"
      },
      "source": [
        "# Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOzzfLNFCtW"
      },
      "source": [
        "We can embed our inputs using PyTorch's [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tHb3v_KH53e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210e85b2-8afb-4117-db73-5440d0375aa4"
      },
      "source": [
        "# Input\n",
        "vocab_size = 10\n",
        "x = torch.randint(high=vocab_size, size=(1,5))\n",
        "print (x)\n",
        "print (x.shape)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2, 6, 5, 2, 6]])\n",
            "torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXUpmH7AFOJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4bd21a-200d-4f6f-ad9b-f83db9995eef"
      },
      "source": [
        "# Embedding layer\n",
        "embeddings = nn.Embedding(embedding_dim=100, num_embeddings=vocab_size)\n",
        "print (embeddings.weight.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVGWIgEGGmHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b916913-0bdb-4bc6-8237-a8a343467321"
      },
      "source": [
        "# Embed the input\n",
        "embeddings(x).shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbO8HYjaGxZY"
      },
      "source": [
        "Each token in the input is represented via embeddings (all out-of-vocabulary (OOV) tokens are given the embedding for `UNK` token.) In the model below, we'll see how to set these embeddings to be pretrained GloVe embeddings and how to choose whether to freeze (fixed embedding weights) those embeddings or not during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTWMME1VmaTQ"
      },
      "source": [
        "# Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu5gHg_Fmzdp"
      },
      "source": [
        "Our inputs are all of varying length but we need each batch to be uniformly shaped. Therefore, we will use padding to make all the inputs in the batch the same length. Our padding index will be 0 (note that this is consistent with the `<PAD>` token defined in our `Tokenizer`).\n",
        "\n",
        "> While embedding our input tokens will create a batch of shape (`N`, `max_seq_len`, `embed_dim`) we only need to provide a 2D matrix (`N`, `max_seq_len`) for using embeddings with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJE5dW33mHZn"
      },
      "source": [
        "def pad_sequences(sequences, max_seq_len=0):\n",
        "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
        "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
        "    #print (max_seq_len) \n",
        "    padded_sequences = np.zeros((len(sequences), max_seq_len))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        padded_sequences[i][:len(sequence)] = sequence\n",
        "    return padded_sequences"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXshHL7Wq74V",
        "outputId": "429f943d-bb11-4a6a-cced-6756f982c95a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train[0:3]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([  16, 1491,  285,  142,  114,   24]),\n",
              " array([1445,   23,  656, 2197,    1]),\n",
              " array([ 120,   14, 1955, 1005, 1529, 4014])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5niX_T9ZmHcn",
        "outputId": "d6035c2e-5a57-4a05-a9f5-f3d62522c032"
      },
      "source": [
        "# 2D sequences\n",
        "padded = pad_sequences(X_train[0:3])\n",
        "print (padded.shape)\n",
        "print (padded)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 6)\n",
            "[[1.600e+01 1.491e+03 2.850e+02 1.420e+02 1.140e+02 2.400e+01]\n",
            " [1.445e+03 2.300e+01 6.560e+02 2.197e+03 1.000e+00 0.000e+00]\n",
            " [1.200e+02 1.400e+01 1.955e+03 1.005e+03 1.529e+03 4.014e+03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8LGKXCmUgzV"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACBKJ77TVBpi"
      },
      "source": [
        "We're going to create Datasets and DataLoaders to be able to efficiently create batches with our data splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jREEFz72Hssx"
      },
      "source": [
        "FILTER_SIZES = list(range(1, 4)) # uni, bi and tri grams"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K0D-vTGUgHV"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y, max_filter_size):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.max_filter_size = max_filter_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<Dataset(N={len(self)})>\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.X[index]\n",
        "        y = self.y[index]\n",
        "        return [X, y]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"Processing on a batch.\"\"\"\n",
        "        # Get inputs\n",
        "        batch = np.array(batch, dtype=object)\n",
        "        X = batch[:, 0]\n",
        "        y = np.stack(batch[:, 1], axis=0)\n",
        "\n",
        "        # Pad sequences\n",
        "        X = pad_sequences(X)\n",
        "\n",
        "        # Cast\n",
        "        X = torch.LongTensor(X.astype(np.int32))\n",
        "        y = torch.LongTensor(y.astype(np.int32))\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n",
        "            shuffle=shuffle, drop_last=drop_last, pin_memory=True)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdAyMvfnUgP9",
        "outputId": "e121365e-a9ba-40b8-c0de-6f9457b79466"
      },
      "source": [
        "# Create datasets\n",
        "max_filter_size = max(FILTER_SIZES)\n",
        "train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=max_filter_size)\n",
        "val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=max_filter_size)\n",
        "test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=max_filter_size)\n",
        "print (\"Datasets:\\n\"\n",
        "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
        "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
        "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
        "    \"Sample point:\\n\"\n",
        "    f\"  X: {train_dataset[0][0]}\\n\"\n",
        "    f\"  y: {train_dataset[0][1]}\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datasets:\n",
            "  Train dataset:<Dataset(N=84000)>\n",
            "  Val dataset: <Dataset(N=18000)>\n",
            "  Test dataset: <Dataset(N=18000)>\n",
            "Sample point:\n",
            "  X: [  16 1491  285  142  114   24]\n",
            "  y: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeK0C3ORUgTu",
        "outputId": "7c423ee5-3671-4cf0-ab7b-1625ff2feb89"
      },
      "source": [
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)\n",
        "val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\n",
        "test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)\n",
        "batch_X, batch_y = next(iter(train_dataloader))\n",
        "print (\"Sample batch:\\n\"\n",
        "    f\"  X: {list(batch_X.size())}\\n\"\n",
        "    f\"  y: {list(batch_y.size())}\\n\"\n",
        "    \"Sample point:\\n\"\n",
        "    f\"  X: {batch_X[0]}\\n\"\n",
        "    f\"  y: {batch_y[0]}\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample batch:\n",
            "  X: [64, 14]\n",
            "  y: [64]\n",
            "Sample point:\n",
            "  X: tensor([  16, 1491,  285,  142,  114,   24,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0], device='cpu')\n",
            "  y: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfhjWZRD94hK"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0U0V8fViHZc"
      },
      "source": [
        "We'll be using a convolutional neural network on top of our embedded tokens to extract meaningful spatial signal. This time, we'll be using many filter widths to act as n-gram feature extractors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goDI3dSxHbgr"
      },
      "source": [
        "Let's visualize the model's forward pass.\n",
        "\n",
        "1. We'll first tokenize our inputs (`batch_size`, `max_seq_len`).\n",
        "2. Then we'll embed our tokenized inputs (`batch_size`, `max_seq_len`, `embedding_dim`).\n",
        "3. We'll apply convolution via filters (`filter_size`, `vocab_size`, `num_filters`) followed by batch normalization. Our filters act as character level n-gram detecors. We have three different filter sizes (2, 3 and 4) and they will act as bi-gram, tri-gram and 4-gram feature extractors, respectivelyy. \n",
        "4. We'll apply 1D global max pooling which will extract the most relevant information from the feature maps for making the decision.\n",
        "5. We feed the pool outputs to a fully-connected (FC) layer (with dropout).\n",
        "6. We use one more FC layer with softmax to derive class probabilities. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIheSuazHeBT"
      },
      "source": [
        "<div align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/basics/embeddings/model.png\" width=\"1000\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I3dmAFtsfy6"
      },
      "source": [
        "import math\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1rRdLydmjdp"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 100\n",
        "DROPOUT_P = 0.1"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juRjat3CiShK"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, num_filters, \n",
        "                 filter_sizes, hidden_dim, dropout_p, num_classes, \n",
        "                 pretrained_embeddings=None, freeze_embeddings=False,\n",
        "                 padding_idx=0):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Filter sizes\n",
        "        self.filter_sizes = filter_sizes\n",
        "        \n",
        "        # Initialize embeddings\n",
        "        if pretrained_embeddings is None:\n",
        "            self.embeddings = nn.Embedding(\n",
        "                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
        "                padding_idx=padding_idx)\n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.embeddings = nn.Embedding(\n",
        "                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
        "                padding_idx=padding_idx, _weight=pretrained_embeddings)\n",
        "        \n",
        "        # Freeze embeddings or not\n",
        "        if freeze_embeddings:\n",
        "            self.embeddings.weight.requires_grad = False\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList(\n",
        "            [nn.Conv1d(in_channels=embedding_dim, \n",
        "                       out_channels=num_filters, \n",
        "                       kernel_size=f) for f in filter_sizes])\n",
        "     \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Embed\n",
        "        x_in, = inputs\n",
        "        x_in = self.embeddings(x_in)\n",
        "\n",
        "        # Rearrange input so num_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x_in = x_in.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z = []\n",
        "        max_seq_len = x_in.shape[2]\n",
        "        for i, f in enumerate(self.filter_sizes):\n",
        "            # `SAME` padding\n",
        "            padding_left = int((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\n",
        "            padding_right = int(math.ceil((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\n",
        "\n",
        "            # Conv + pool\n",
        "            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))\n",
        "            _z = F.max_pool1d(_z, _z.size(2)).squeeze(2)\n",
        "            z.append(_z)\n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat(z, 1)\n",
        "\n",
        "        # FC layers\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBmYu6wjkgf0"
      },
      "source": [
        "# GloVe embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFRaj2AUojN5"
      },
      "source": [
        "We're going create some utility functions to be able to load the pretrained GloVe embeddings into our Embeddings layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9uev5AGsuqq"
      },
      "source": [
        "def load_glove_embeddings(embeddings_file):\n",
        "    \"\"\"Load embeddings from a file.\"\"\"\n",
        "    embeddings = {}\n",
        "    with open(embeddings_file, \"r\") as fp:\n",
        "        for index, line in enumerate(fp):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = embedding\n",
        "    return embeddings"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQHD-ThwWnjD"
      },
      "source": [
        "def make_embeddings_matrix(embeddings, word_index, embedding_dim):\n",
        "    \"\"\"Create embeddings matrix to use in Embedding layer.\"\"\"\n",
        "    embedding_matrix = np.zeros((len(word_index), embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WxP2GR3LmrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dded3b2-9343-4035-954b-a61620fb5f6d"
      },
      "source": [
        "# Create embeddings\n",
        "embeddings_file = 'glove.6B.{0}d.txt'.format(EMBEDDING_DIM)\n",
        "glove_embeddings = load_glove_embeddings(embeddings_file=embeddings_file)\n",
        "embedding_matrix = make_embeddings_matrix(\n",
        "    embeddings=glove_embeddings, word_index=tokenizer.token_to_index, \n",
        "    embedding_dim=EMBEDDING_DIM)\n",
        "print (f\"<Embeddings(words={embedding_matrix.shape[0]}, dim={embedding_matrix.shape[1]})>\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Embeddings(words=5000, dim=100)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C26maF-9Goit"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTWQcUJ_GrIx"
      },
      "source": [
        "We have first have to decice whether to use pretrained embeddings randomly initialized ones. Then, we can choose to freeze our embeddings or continue to train them using the supervised data (this could lead to overfitting). Here are the three experiments we're going to conduct: \n",
        "* randomly initialized embeddings (fine-tuned)\n",
        "* GloVe embeddings (frozen)\n",
        "* GloVe embeddings (fine-tuned)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geKOPVzVK6S9"
      },
      "source": [
        "import json\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from torch.optim import Adam"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64iPmq2lDv2h"
      },
      "source": [
        "NUM_FILTERS = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "PATIENCE = 5\n",
        "NUM_EPOCHS = 10"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIXt8XA09vYX"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n",
        "\n",
        "        # Set params\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train_step(self, dataloader):\n",
        "        \"\"\"Train step.\"\"\"\n",
        "        # Set model to train mode\n",
        "        self.model.train()\n",
        "        loss = 0.0\n",
        "\n",
        "        # Iterate over train batches\n",
        "        for i, batch in enumerate(dataloader):\n",
        "\n",
        "            # Step\n",
        "            batch = [item.to(self.device) for item in batch]  # Set device\n",
        "            inputs, targets = batch[:-1], batch[-1]\n",
        "            self.optimizer.zero_grad()  # Reset gradients\n",
        "            z = self.model(inputs)  # Forward pass\n",
        "            J = self.loss_fn(z, targets)  # Define loss\n",
        "            J.backward()  # Backward pass\n",
        "            self.optimizer.step()  # Update weights\n",
        "\n",
        "            # Cumulative Metrics\n",
        "            loss += (J.detach().item() - loss) / (i + 1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def eval_step(self, dataloader):\n",
        "        \"\"\"Validation or test step.\"\"\"\n",
        "        # Set model to eval mode\n",
        "        self.model.eval()\n",
        "        loss = 0.0\n",
        "        y_trues, y_probs = [], []\n",
        "\n",
        "        # Iterate over val batches\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(dataloader):\n",
        "\n",
        "                # Step\n",
        "                batch = [item.to(self.device) for item in batch]  # Set device\n",
        "                inputs, y_true = batch[:-1], batch[-1]\n",
        "                z = self.model(inputs)  # Forward pass\n",
        "                J = self.loss_fn(z, y_true).item()\n",
        "\n",
        "                # Cumulative Metrics\n",
        "                loss += (J - loss) / (i + 1)\n",
        "\n",
        "                # Store outputs\n",
        "                y_prob = torch.sigmoid(z).cpu().numpy()\n",
        "                y_probs.extend(y_prob)\n",
        "                y_trues.extend(y_true.cpu().numpy())\n",
        "\n",
        "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
        "\n",
        "    def predict_step(self, dataloader):\n",
        "        \"\"\"Prediction step.\"\"\"\n",
        "        # Set model to eval mode\n",
        "        self.model.eval()\n",
        "        y_probs = []\n",
        "\n",
        "        # Iterate over val batches\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(dataloader):\n",
        "\n",
        "                # Forward pass w/ inputs\n",
        "                inputs, targets = batch[:-1], batch[-1]\n",
        "                y_prob = self.model(inputs, apply_softmax=True)\n",
        "\n",
        "                # Store outputs\n",
        "                y_probs.extend(y_prob)\n",
        "\n",
        "        return np.vstack(y_probs)\n",
        "    \n",
        "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
        "        best_val_loss = np.inf\n",
        "        for epoch in range(num_epochs):\n",
        "            # Steps\n",
        "            train_loss = self.train_step(dataloader=train_dataloader)\n",
        "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = self.model\n",
        "                _patience = patience  # reset _patience\n",
        "            else:\n",
        "                _patience -= 1\n",
        "            if not _patience:  # 0\n",
        "                print(\"Stopping early!\")\n",
        "                break\n",
        "\n",
        "            # Logging\n",
        "            print(\n",
        "                f\"Epoch: {epoch+1} | \"\n",
        "                f\"train_loss: {train_loss:.5f}, \"\n",
        "                f\"val_loss: {val_loss:.5f}, \"\n",
        "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
        "                f\"_patience: {_patience}\"\n",
        "            )\n",
        "        return best_model"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us7Smprz9cWO"
      },
      "source": [
        "def get_performance(y_true, y_pred, classes):\n",
        "    \"\"\"Per-class performance metrics.\"\"\"\n",
        "    # Performance\n",
        "    performance = {\"overall\": {}, \"class\": {}}\n",
        "\n",
        "    # Overall performance\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "    performance[\"overall\"][\"precision\"] = metrics[0]\n",
        "    performance[\"overall\"][\"recall\"] = metrics[1]\n",
        "    performance[\"overall\"][\"f1\"] = metrics[2]\n",
        "    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n",
        "\n",
        "    # Per-class performance\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "    for i in range(len(classes)):\n",
        "        performance[\"class\"][classes[i]] = {\n",
        "            \"precision\": metrics[0][i],\n",
        "            \"recall\": metrics[1][i],\n",
        "            \"f1\": metrics[2][i],\n",
        "            \"num_samples\": np.float64(metrics[3][i]),\n",
        "        }\n",
        "\n",
        "    return performance"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8JzMrcv_p8a"
      },
      "source": [
        "### Randomly initialized embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnLSYV0WKo8x"
      },
      "source": [
        "PRETRAINED_EMBEDDINGS = None\n",
        "FREEZE_EMBEDDINGS = False"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD4sRUS5_lwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280f8c2a-4b63-4d74-8933-458c21ca7de3"
      },
      "source": [
        "# Initialize model\n",
        "model = CNN(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n",
        "    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n",
        "    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\n",
        "model = model.to(device) # set device\n",
        "print (model.named_parameters)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of CNN(\n",
            "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n",
            "    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n",
            "    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=150, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uiqDFypJLU9"
      },
      "source": [
        "# Define Loss\n",
        "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVLmJFYFJLXs"
      },
      "source": [
        "# Define optimizer & scheduler\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE) \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=3)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaV-uU0tJLbI"
      },
      "source": [
        "# Trainer module\n",
        "trainer = Trainer(\n",
        "    model=model, device=device, loss_fn=loss_fn, \n",
        "    optimizer=optimizer, scheduler=scheduler)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ee8qqUnJLeg",
        "outputId": "ddab427d-2aac-4b1f-9d23-eb52e3744c47"
      },
      "source": [
        "# Train\n",
        "best_model = trainer.train(\n",
        "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 0.76470, val_loss: 0.58878, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 2 | train_loss: 0.49049, val_loss: 0.55462, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 3 | train_loss: 0.40273, val_loss: 0.56453, lr: 1.00E-03, _patience: 4\n",
            "Epoch: 4 | train_loss: 0.34299, val_loss: 0.60008, lr: 1.00E-03, _patience: 3\n",
            "Epoch: 5 | train_loss: 0.29478, val_loss: 0.64381, lr: 1.00E-03, _patience: 2\n",
            "Epoch: 6 | train_loss: 0.25439, val_loss: 0.69849, lr: 1.00E-04, _patience: 1\n",
            "Stopping early!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRzw6CpqJRUA"
      },
      "source": [
        "# Get predictions\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYUvB6FlJRW1",
        "outputId": "da1c13ed-a4b4-470c-b4fe-1f9878cfc1dd"
      },
      "source": [
        "# Determine performance\n",
        "performance = get_performance(\n",
        "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
        "print (json.dumps(performance['overall'], indent=2))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"precision\": 0.8110521155131828,\n",
            "  \"recall\": 0.8103888888888889,\n",
            "  \"f1\": 0.8104908098537666,\n",
            "  \"num_samples\": 18000.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To_CB7ibLesP"
      },
      "source": [
        "### GloVe embeddings (frozen)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT9w__AMkqfG"
      },
      "source": [
        "PRETRAINED_EMBEDDINGS = embedding_matrix\n",
        "FREEZE_EMBEDDINGS = True"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg13AyoUkqcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b587c3-ff96-4c5a-97de-1e1a903d0748"
      },
      "source": [
        "# Initialize model\n",
        "model = CNN(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n",
        "    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n",
        "    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\n",
        "model = model.to(device) # set device\n",
        "print (model.named_parameters)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of CNN(\n",
            "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n",
            "    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n",
            "    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=150, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rJNp4Vb-dqz"
      },
      "source": [
        "# Define Loss\n",
        "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKtdPOdM-dt0"
      },
      "source": [
        "# Define optimizer & scheduler\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE) \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=3)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtVthG4r-dwy"
      },
      "source": [
        "# Trainer module\n",
        "trainer = Trainer(\n",
        "    model=model, device=device, loss_fn=loss_fn, \n",
        "    optimizer=optimizer, scheduler=scheduler)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy3FP3ht-gJY",
        "outputId": "0ed3e972-4688-46a7-d888-cd69855d77cb"
      },
      "source": [
        "# Train\n",
        "best_model = trainer.train(\n",
        "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 0.51719, val_loss: 0.47553, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 2 | train_loss: 0.44475, val_loss: 0.46385, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 3 | train_loss: 0.41411, val_loss: 0.46227, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 4 | train_loss: 0.38947, val_loss: 0.46821, lr: 1.00E-03, _patience: 4\n",
            "Epoch: 5 | train_loss: 0.37015, val_loss: 0.47740, lr: 1.00E-03, _patience: 3\n",
            "Epoch: 6 | train_loss: 0.35430, val_loss: 0.48635, lr: 1.00E-03, _patience: 2\n",
            "Epoch: 7 | train_loss: 0.33993, val_loss: 0.49891, lr: 1.00E-04, _patience: 1\n",
            "Stopping early!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2e0q965-gMK"
      },
      "source": [
        "# Get predictions\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyNhVcHi-juw",
        "outputId": "7838f67e-9482-4904-f162-cfba1ed64e6b"
      },
      "source": [
        "# Determine performance\n",
        "performance = get_performance(\n",
        "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
        "print (json.dumps(performance['overall'], indent=2))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"precision\": 0.8318422616810701,\n",
            "  \"recall\": 0.8316666666666667,\n",
            "  \"f1\": 0.831559860830077,\n",
            "  \"num_samples\": 18000.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUVkeDbNqO7V"
      },
      "source": [
        "### Fine-tuned GloVe embeddings (unfrozen)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eubLrHydkt_J"
      },
      "source": [
        "PRETRAINED_EMBEDDINGS = embedding_matrix\n",
        "FREEZE_EMBEDDINGS = False"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGeZwoy9qUpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b50374-ec98-4093-a2e9-4b0f2e8b367c"
      },
      "source": [
        "# Initialize model\n",
        "model = CNN(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n",
        "    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n",
        "    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\n",
        "model = model.to(device) # set device\n",
        "print (model.named_parameters)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of CNN(\n",
            "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n",
            "    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n",
            "    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=150, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifqXyPZ1JKWY"
      },
      "source": [
        "# Define Loss\n",
        "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXGrQ0ceJKZk"
      },
      "source": [
        "# Define optimizer & scheduler\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE) \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=3)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IinLK_ohJKdr"
      },
      "source": [
        "# Trainer module\n",
        "trainer = Trainer(\n",
        "    model=model, device=device, loss_fn=loss_fn, \n",
        "    optimizer=optimizer, scheduler=scheduler)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpVOifjMJKgx",
        "outputId": "02c16160-02e2-456d-93cd-31e89ac55cc6"
      },
      "source": [
        "# Train\n",
        "best_model = trainer.train(\n",
        "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 0.48830, val_loss: 0.44469, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 2 | train_loss: 0.39016, val_loss: 0.44237, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 3 | train_loss: 0.34386, val_loss: 0.45747, lr: 1.00E-03, _patience: 4\n",
            "Epoch: 4 | train_loss: 0.30052, val_loss: 0.49225, lr: 1.00E-03, _patience: 3\n",
            "Epoch: 5 | train_loss: 0.25620, val_loss: 0.54176, lr: 1.00E-03, _patience: 2\n",
            "Epoch: 6 | train_loss: 0.21255, val_loss: 0.61870, lr: 1.00E-04, _patience: 1\n",
            "Stopping early!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJfbNqp2JQgT"
      },
      "source": [
        "# Get predictions\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thdmnUOTJQld",
        "outputId": "b65f4caa-edcc-4bf7-fada-4d70eb73ede7"
      },
      "source": [
        "# Determine performance\n",
        "performance = get_performance(\n",
        "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
        "print (json.dumps(performance['overall'], indent=2))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"precision\": 0.8296263097192519,\n",
            "  \"recall\": 0.8288888888888889,\n",
            "  \"f1\": 0.8289624818893484,\n",
            "  \"num_samples\": 18000.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R-df_DMY51A"
      },
      "source": [
        "# Save artifacts\n",
        "from pathlib import Path\n",
        "dir = Path(\"cnn\")\n",
        "dir.mkdir(parents=True, exist_ok=True)\n",
        "label_encoder.save(fp=Path(dir, 'label_encoder.json'))\n",
        "tokenizer.save(fp=Path(dir, 'tokenizer.json'))\n",
        "torch.save(best_model.state_dict(), Path(dir, 'model.pt'))\n",
        "with open(Path(dir, 'performance.json'), \"w\") as fp:\n",
        "    json.dump(performance, indent=2, sort_keys=False, fp=fp)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKdgLetZKhEj"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPWRyqBoKks0"
      },
      "source": [
        "def get_probability_distribution(y_prob, classes):\n",
        "    \"\"\"Create a dict of class probabilities from an array.\"\"\"\n",
        "    results = {}\n",
        "    for i, class_ in enumerate(classes):\n",
        "        results[class_] = np.float64(y_prob[i])\n",
        "    sorted_results = {k: v for k, v in sorted(\n",
        "        results.items(), key=lambda item: item[1], reverse=True)}\n",
        "    return sorted_results"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zg0ErQMY4cZ",
        "outputId": "2204c98d-31e0-4848-81d3-1e534f021fa5"
      },
      "source": [
        "# Load artifacts\n",
        "device = torch.device(\"cpu\")\n",
        "label_encoder = LabelEncoder.load(fp=Path(dir, 'label_encoder.json'))\n",
        "tokenizer = Tokenizer.load(fp=Path(dir, 'tokenizer.json'))\n",
        "model = CNN(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n",
        "    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n",
        "    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\n",
        "model.load_state_dict(torch.load(Path(dir, 'model.pt'), map_location=device))\n",
        "model.to(device)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
              "  (conv): ModuleList(\n",
              "    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n",
              "    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n",
              "    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=150, out_features=100, bias=True)\n",
              "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bviv-K-FY4gS"
      },
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(model=model, device=device)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDD44HKfY4jY",
        "outputId": "2804eb7a-3631-4b0d-c259-167ecfeb5fdd"
      },
      "source": [
        "# Dataloader\n",
        "text = [\"The final tennis tournament starts next week.\"]\n",
        "X = tokenizer.texts_to_sequences([preprocess(tex) for tex in text ])\n",
        "print (tokenizer.sequences_to_texts(X))\n",
        "y_filler = label_encoder.encode([label_encoder.classes[0]]*len(X))\n",
        "dataset = Dataset(X=X, y=y_filler, max_filter_size=max_filter_size)\n",
        "dataloader = dataset.create_dataloader(batch_size=batch_size)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['final tennis tournament starts next week']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXrACSa6ZJgb",
        "outputId": "3f815fe5-be6a-4e79-d026-278cc3b919e3"
      },
      "source": [
        "# Inference\n",
        "y_prob = trainer.predict_step(dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "label_encoder.decode(y_pred)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sports']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbz0ZnYSZJkD",
        "outputId": "5b38d72d-5523-4822-9cb6-9b33c5973670"
      },
      "source": [
        "# Class distributions\n",
        "prob_dist = get_probability_distribution(y_prob=y_prob[0], classes=label_encoder.classes)\n",
        "print (json.dumps(prob_dist, indent=2))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"Sports\": 0.9999988079071045,\n",
            "  \"World\": 1.2448068673620583e-06,\n",
            "  \"Sci/Tech\": 2.5783819523894635e-10,\n",
            "  \"Business\": 1.464014846852546e-11\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj-_P_HpKgf5",
        "outputId": "2305e5d5-fcde-45ae-fd3f-c5d47f6dabf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_prob"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4640148e-11, 2.5783820e-10, 9.9999881e-01, 1.2448069e-06]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXZtx3nsKlDr"
      },
      "source": [
        "# Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KImhaLkcFuJ"
      },
      "source": [
        "We went through all the trouble of padding our inputs before convolution to result is outputs of the same shape as our inputs so we can try to get some interpretability. Since every token is mapped to a convolutional output on which we apply max pooling, we can see which token's output was most influential towards the prediction. We first need to get the conv outputs from our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnHVnI8hdxYC"
      },
      "source": [
        "import collections\n",
        "import seaborn as sns"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atX4qOs1Kl1Y"
      },
      "source": [
        "class InterpretableCNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, num_filters, \n",
        "                 filter_sizes, hidden_dim, dropout_p, num_classes, \n",
        "                 pretrained_embeddings=None, freeze_embeddings=False,\n",
        "                 padding_idx=0):\n",
        "        super(InterpretableCNN, self).__init__()\n",
        "\n",
        "        # Filter sizes\n",
        "        self.filter_sizes = filter_sizes\n",
        "        \n",
        "        # Initialize embeddings\n",
        "        if pretrained_embeddings is None:\n",
        "            self.embeddings = nn.Embedding(\n",
        "                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
        "                padding_idx=padding_idx)\n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.embeddings = nn.Embedding(\n",
        "                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
        "                padding_idx=padding_idx, _weight=pretrained_embeddings)\n",
        "        \n",
        "        # Freeze embeddings or not\n",
        "        if freeze_embeddings:\n",
        "            self.embeddings.weight.requires_grad = False\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList(\n",
        "            [nn.Conv1d(in_channels=embedding_dim, \n",
        "                       out_channels=num_filters, \n",
        "                       kernel_size=f) for f in filter_sizes])\n",
        "     \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Embed\n",
        "        x_in, = inputs\n",
        "        x_in = self.embeddings(x_in)\n",
        "\n",
        "        # Rearrange input so num_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x_in = x_in.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z = []\n",
        "        max_seq_len = x_in.shape[2]\n",
        "        for i, f in enumerate(self.filter_sizes):\n",
        "            # `SAME` padding\n",
        "            padding_left = int((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\n",
        "            padding_right = int(math.ceil((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\n",
        "\n",
        "            # Conv + pool\n",
        "            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))\n",
        "            print (_z.shape)\n",
        "            z.append(_z.cpu().numpy())\n",
        "        \n",
        "        return z"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wybIldYFctMF"
      },
      "source": [
        "PRETRAINED_EMBEDDINGS = embedding_matrix\n",
        "FREEZE_EMBEDDINGS = False"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLNK4ez2ctPB",
        "outputId": "164122f7-d807-4677-f082-1e047c596b74"
      },
      "source": [
        "# Initialize model\n",
        "interpretable_model = InterpretableCNN(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n",
        "    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n",
        "    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\n",
        "interpretable_model.load_state_dict(torch.load(Path(dir, 'model.pt'), map_location=device))\n",
        "interpretable_model.to(device)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InterpretableCNN(\n",
              "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
              "  (conv): ModuleList(\n",
              "    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n",
              "    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n",
              "    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=150, out_features=100, bias=True)\n",
              "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGdju-O6dEwW"
      },
      "source": [
        "# Initialize trainer\n",
        "interpretable_trainer = Trainer(model=interpretable_model, device=device)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NJYmV6idE1v",
        "outputId": "12c7ed56-28c1-4c03-e89f-449c27ab9ad8"
      },
      "source": [
        "# Get conv outputs\n",
        "conv_outputs = interpretable_trainer.predict_step(dataloader)\n",
        "print (conv_outputs.shape) # (len(filter_sizes), num_filters, max_seq_len)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50, 6])\n",
            "torch.Size([1, 50, 6])\n",
            "torch.Size([1, 50, 6])\n",
            "(3, 50, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXhZcZVRRY15",
        "outputId": "ef51003c-ed9d-404a-abdb-8545fafae800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "ui.shape"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A80SRr_IH7A",
        "outputId": "82099b36-0b1f-4e73-e1ac-39ff0d8cfadf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "q=[]\n",
        "ds=[]\n",
        "ui = torch.Tensor([ [[12,87,3],[6,9,2]] ])\n",
        "ni= torch.Tensor([[[2,8,33],[67,91,29]]])\n",
        "q.append(ui)\n",
        "q.append(ni)\n",
        "sm=torch.cat(q,1)\n",
        "sm.shape"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVbj84MeLOxR",
        "outputId": "e049eab6-0273-4ea7-9733-19122f6c2d88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.vstack(q).shape"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmKbT1MKHnvQ",
        "outputId": "08fb33df-0ea6-475c-fb14-a303be9476a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_outputs"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-1.91519213e+00, -1.81134844e+00, -2.25799990e+00,\n",
              "         -1.03602326e+00, -8.64143014e-01, -1.07130158e+00],\n",
              "        [-1.88178122e-01,  1.20138288e+00,  1.04590762e+00,\n",
              "         -5.08833885e-01, -2.15984732e-02, -1.89433783e-01],\n",
              "        [-1.27502406e+00, -1.96213245e+00, -1.29597747e+00,\n",
              "         -8.52925897e-01, -6.88528299e-01, -8.07564318e-01],\n",
              "        [-3.88531595e-01,  1.88637719e-01, -1.93000734e-02,\n",
              "         -7.31524587e-01, -8.12450469e-01, -4.49786961e-01],\n",
              "        [-7.18757331e-01, -4.83327359e-01, -2.81042874e-01,\n",
              "         -1.55944300e+00, -1.16543806e+00, -1.92584550e+00],\n",
              "        [-1.86064637e+00, -2.06429315e+00, -1.67798233e+00,\n",
              "         -8.34992051e-01, -7.68985271e-01, -1.57579219e+00],\n",
              "        [-5.69532871e-01, -1.17562091e+00, -1.20146883e+00,\n",
              "         -4.01965350e-01, -1.08103871e+00, -4.00821686e-01],\n",
              "        [-2.09944820e+00, -1.28633070e+00, -2.19147372e+00,\n",
              "         -7.15820670e-01, -6.51509404e-01, -7.90217042e-01],\n",
              "        [-4.99225497e-01,  6.20745242e-01, -9.25249681e-02,\n",
              "         -6.77358031e-01, -1.02329338e+00, -1.13959730e+00],\n",
              "        [-6.53920650e-01, -1.14814150e+00, -5.06985843e-01,\n",
              "         -6.47771180e-01, -8.31494033e-02, -8.14744651e-01],\n",
              "        [ 2.22003222e-01,  5.05068421e-01,  7.79031098e-01,\n",
              "         -3.06901693e-01,  7.74263740e-02, -2.20927149e-01],\n",
              "        [-8.54808927e-01, -1.63309550e+00, -8.49238992e-01,\n",
              "         -7.88089991e-01, -6.47936225e-01, -7.78754711e-01],\n",
              "        [ 4.39308554e-01, -1.36120722e-01,  5.35081863e-01,\n",
              "          3.92136574e-01, -1.59854472e-01,  1.24668367e-02],\n",
              "        [-5.70602059e-01, -4.73053783e-01, -8.09741497e-01,\n",
              "         -7.24309146e-01, -6.70563996e-01, -1.13784230e+00],\n",
              "        [-4.74229574e-01, -2.07187176e-01, -5.43352902e-01,\n",
              "         -5.74973643e-01, -1.12053275e+00, -9.92965996e-01],\n",
              "        [-8.00256848e-01, -1.43028784e+00, -1.07971370e+00,\n",
              "         -8.45551729e-01, -7.67882764e-01, -1.34138238e+00],\n",
              "        [-1.00780511e+00, -2.85089910e-01, -6.96049929e-01,\n",
              "         -1.25215864e+00, -1.22528923e+00, -1.49581742e+00],\n",
              "        [-1.27789581e+00, -8.75040233e-01, -8.65847230e-01,\n",
              "         -4.09463525e-01, -7.25232363e-01,  3.75696063e-01],\n",
              "        [-4.32590544e-01,  1.97065949e-01,  9.84026194e-02,\n",
              "         -5.14259458e-01, -6.38025582e-01, -7.74164379e-01],\n",
              "        [ 1.41610488e-01, -1.35913157e+00, -4.04398531e-01,\n",
              "         -7.21443057e-01, -5.08510470e-01, -9.02069509e-01],\n",
              "        [-3.07841569e-01,  6.20498359e-01,  3.36082071e-01,\n",
              "          4.17459086e-02, -9.21589196e-01, -8.71851027e-01],\n",
              "        [-6.50690138e-01, -8.75886858e-01, -6.89913094e-01,\n",
              "         -5.94403893e-02, -8.15640539e-02,  6.21293858e-02],\n",
              "        [-1.52937698e+00, -3.57229233e-01, -5.79285681e-01,\n",
              "         -1.06523299e+00, -1.02714932e+00, -1.09969449e+00],\n",
              "        [-4.48532373e-01, -2.32262179e-01,  4.51723397e-01,\n",
              "         -4.52853084e-01, -6.45680785e-01, -7.14325786e-01],\n",
              "        [-7.59864271e-01, -7.23869130e-02, -8.57756257e-01,\n",
              "         -9.29888308e-01, -7.66530216e-01, -6.33023918e-01],\n",
              "        [-1.33639061e+00, -4.90361452e-01, -1.34982312e+00,\n",
              "         -1.12538457e+00, -5.11237562e-01, -5.40032923e-01],\n",
              "        [-1.96083760e+00, -3.01019812e+00, -3.10817289e+00,\n",
              "         -1.15424860e+00, -7.99239874e-01, -1.34378040e+00],\n",
              "        [-2.74005353e-01,  4.03591692e-01,  3.58916610e-01,\n",
              "          4.74456027e-02, -7.52535284e-01, -1.11521280e+00],\n",
              "        [-1.21516740e+00, -7.83714473e-01, -8.38106871e-01,\n",
              "         -8.09794962e-01, -8.83334816e-01, -9.68495488e-01],\n",
              "        [-2.40578175e-01,  4.75728095e-01,  1.06008217e-01,\n",
              "          1.71190798e-02, -4.05010462e-01, -8.12058389e-01],\n",
              "        [-7.38227487e-01,  6.96698070e-01,  2.82404453e-01,\n",
              "         -4.13229614e-01, -1.13946772e+00, -8.77442539e-01],\n",
              "        [-7.20579982e-01, -7.59947956e-01, -1.24383938e+00,\n",
              "         -3.31096113e-01, -9.00911093e-01, -5.73035419e-01],\n",
              "        [-1.06112230e+00, -1.43936181e+00, -1.13081515e+00,\n",
              "         -1.01581728e+00, -4.31939334e-01, -4.58380997e-01],\n",
              "        [ 1.52107224e-01,  2.05457136e-01, -4.48756516e-01,\n",
              "         -3.40341002e-01, -6.21252298e-01, -9.91715550e-01],\n",
              "        [-1.74858594e+00, -1.70387208e+00, -1.67953742e+00,\n",
              "         -1.51861477e+00, -1.01581895e+00, -1.45457530e+00],\n",
              "        [-1.05194068e+00, -1.68302083e+00, -1.66585135e+00,\n",
              "         -1.02531600e+00, -5.55873871e-01, -1.37060189e+00],\n",
              "        [ 6.49565086e-02,  8.33698690e-01,  3.56644005e-01,\n",
              "         -2.42342845e-01, -6.13268837e-02, -6.28770828e-01],\n",
              "        [-6.79881752e-01,  1.44034314e+00,  4.14808959e-01,\n",
              "         -1.08275259e+00, -1.21751714e+00, -1.08193851e+00],\n",
              "        [-1.49034631e+00, -1.21722567e+00, -1.47808063e+00,\n",
              "         -1.11736047e+00, -5.27510941e-01, -1.22253723e-01],\n",
              "        [-8.43426943e-01,  2.10653767e-01, -7.38635480e-01,\n",
              "         -1.13040805e+00, -1.28144765e+00, -4.57389235e-01],\n",
              "        [ 1.25764042e-01, -8.01622510e-01, -6.00015223e-01,\n",
              "          3.44967663e-01, -1.89917266e-01,  7.14243278e-02],\n",
              "        [ 3.27781647e-01,  1.49721217e+00,  6.78149760e-01,\n",
              "          7.61104584e-01,  1.13374531e-01, -1.88247144e-01],\n",
              "        [-1.24347545e-01, -5.17375469e-01, -6.09243035e-01,\n",
              "         -2.82001436e-01, -2.50605702e-01, -2.91064948e-01],\n",
              "        [-8.98002923e-01, -6.42815232e-02, -7.85650730e-01,\n",
              "         -8.62687528e-01, -1.75144625e+00, -1.36309922e+00],\n",
              "        [-1.89880824e+00, -1.40208340e+00, -2.25908732e+00,\n",
              "         -1.55303848e+00, -1.52354074e+00, -1.39274490e+00],\n",
              "        [-1.40017331e+00, -1.31275928e+00, -1.29122055e+00,\n",
              "         -1.54408348e+00, -8.54319632e-01, -1.19147599e+00],\n",
              "        [ 1.23339891e+00,  2.61445856e+00,  2.98736835e+00,\n",
              "         -6.02634996e-03, -2.64517218e-01,  4.10772949e-01],\n",
              "        [-1.00445890e+00, -7.99132466e-01, -1.37117100e+00,\n",
              "         -2.90676266e-01, -7.19485343e-01, -4.20360476e-01],\n",
              "        [-1.60937810e+00, -1.27558208e+00, -1.90081465e+00,\n",
              "         -1.21146524e+00, -1.42442298e+00, -1.77723265e+00],\n",
              "        [ 1.57385677e-01,  1.16718888e+00,  1.22476435e+00,\n",
              "         -8.20964873e-01, -1.06290054e+00, -5.57864010e-01]],\n",
              "\n",
              "       [[-2.87404394e+00, -3.16643071e+00, -2.80222440e+00,\n",
              "         -1.63087988e+00, -1.38134241e+00, -1.14636540e+00],\n",
              "        [-7.74835050e-03,  6.45861328e-01, -6.55388087e-02,\n",
              "         -1.56650531e+00, -8.20171773e-01, -7.30934381e-01],\n",
              "        [ 1.23831164e-02, -1.69296324e+00, -2.23262167e+00,\n",
              "         -2.20327759e+00, -1.38349760e+00, -6.73397303e-01],\n",
              "        [ 6.03052318e-01, -2.48996347e-01, -3.27494353e-01,\n",
              "         -8.38803351e-01, -1.18496990e+00, -5.10789573e-01],\n",
              "        [-5.66944480e-01, -1.44315398e+00, -1.49194419e+00,\n",
              "         -1.19810796e+00, -1.92164624e+00, -4.98899311e-01],\n",
              "        [-2.92221403e+00, -1.79622161e+00, -2.11298800e+00,\n",
              "         -2.08610964e+00, -2.78823853e+00, -1.89042413e+00],\n",
              "        [ 9.28050727e-02,  1.10320568e+00, -5.64336836e-01,\n",
              "         -1.20206404e+00, -5.16675651e-01, -1.57922947e+00],\n",
              "        [-1.49419236e+00, -1.26216567e+00, -1.90413022e+00,\n",
              "         -1.14946282e+00, -1.57058656e+00, -2.47754544e-01],\n",
              "        [-2.92310882e+00, -3.28090429e+00, -2.58981085e+00,\n",
              "         -6.33595407e-01, -2.40101004e+00, -5.28430343e-01],\n",
              "        [-6.78713799e-01, -6.78199410e-01, -5.58341980e-01,\n",
              "         -3.32517684e-01, -7.03578353e-01, -3.05574387e-01],\n",
              "        [-3.18238258e+00, -1.11946344e+00, -1.83339083e+00,\n",
              "         -2.03277922e+00, -3.43213248e+00, -2.75633407e+00],\n",
              "        [-7.12247372e-01, -4.04304564e-02, -1.06698430e+00,\n",
              "         -7.43220568e-01, -4.88004088e-03, -2.06642777e-01],\n",
              "        [-1.36080098e+00, -2.30391932e+00, -2.70381284e+00,\n",
              "         -1.71217549e+00, -2.39441752e+00, -1.85802352e+00],\n",
              "        [ 1.84639084e+00,  1.90497243e+00,  2.04160619e+00,\n",
              "         -3.70584786e-01, -9.33074117e-01, -1.26908779e-01],\n",
              "        [-2.14143491e+00, -1.64714062e+00, -1.47177196e+00,\n",
              "         -2.55239820e+00, -2.09868264e+00, -1.28814256e+00],\n",
              "        [-2.86837649e+00, -2.79444623e+00, -1.97914767e+00,\n",
              "         -1.45047200e+00, -1.04712903e+00, -1.47185290e+00],\n",
              "        [-2.38184214e+00, -1.28157914e+00,  3.59290615e-02,\n",
              "         -3.36935997e-01, -1.04315734e+00, -1.11751354e+00],\n",
              "        [-1.51538730e+00, -2.53373337e+00, -5.26217699e-01,\n",
              "         -6.85109019e-01, -1.37457323e+00, -6.29380822e-01],\n",
              "        [-9.93910193e-01, -1.23696733e+00, -2.21594048e+00,\n",
              "         -1.06535387e+00, -1.69740450e+00, -1.37002790e+00],\n",
              "        [-2.34821725e+00, -2.00823498e+00, -8.33204389e-01,\n",
              "         -1.40062356e+00, -1.09220314e+00, -6.87371969e-01],\n",
              "        [-6.79746985e-01, -8.50971639e-01, -2.01960039e+00,\n",
              "         -1.84761167e+00, -2.34738731e+00, -1.56558716e+00],\n",
              "        [-1.72376490e+00, -1.33623910e+00, -1.81335330e+00,\n",
              "         -2.05739045e+00, -1.73847294e+00, -1.22301078e+00],\n",
              "        [-4.22918797e-01, -2.32789934e-01, -4.72319394e-01,\n",
              "         -7.80542016e-01, -1.51680255e+00, -8.76168847e-01],\n",
              "        [ 8.48589540e-01,  8.91811013e-01,  1.14168301e-02,\n",
              "         -8.42035055e-01,  5.03976464e-01, -8.61991823e-01],\n",
              "        [-1.40496957e+00, -1.40606034e+00, -1.26717579e+00,\n",
              "         -1.51102126e+00, -9.12106752e-01, -1.14404178e+00],\n",
              "        [ 1.16031277e+00,  1.07824731e+00,  1.70376122e+00,\n",
              "          8.61693621e-02, -4.03922319e-01, -9.83416736e-02],\n",
              "        [-9.33039784e-01, -2.04334474e+00, -4.36054856e-01,\n",
              "         -1.79270673e+00, -8.66302609e-01, -1.01680410e+00],\n",
              "        [ 4.59006190e-01,  5.10595560e-01,  1.02092266e+00,\n",
              "          8.79050940e-02,  2.75491536e-01, -4.77756679e-01],\n",
              "        [-2.19173998e-01,  1.25141251e+00,  4.22288418e-01,\n",
              "         -9.53023434e-01, -1.58029544e+00, -1.16855860e+00],\n",
              "        [-5.53525835e-02,  3.48055452e-01,  8.88372421e-01,\n",
              "         -2.14503482e-01, -6.92294717e-01, -6.93492234e-01],\n",
              "        [ 2.05575943e+00,  1.77877605e-01, -7.07217872e-01,\n",
              "          6.88629150e-02, -4.56597388e-01,  1.44885138e-01],\n",
              "        [-8.99909973e-01, -1.19890857e+00, -1.18014181e+00,\n",
              "         -1.77332091e+00, -1.64279616e+00, -1.22658920e+00],\n",
              "        [-1.26939452e+00, -9.58137572e-01, -3.30617499e+00,\n",
              "         -2.43497062e+00, -1.80918074e+00, -1.48361111e+00],\n",
              "        [-9.71971393e-01, -8.77061188e-01, -6.42283976e-01,\n",
              "         -1.41979587e+00, -7.96735108e-01, -1.57695854e+00],\n",
              "        [-4.67515647e-01,  1.36024141e+00,  1.23632586e+00,\n",
              "         -5.53158522e-01, -1.18962955e+00, -7.75804698e-01],\n",
              "        [-1.90960574e+00, -1.92409134e+00, -1.31038976e+00,\n",
              "         -1.26825500e+00, -4.70941961e-01, -2.23308593e-01],\n",
              "        [-1.75159526e+00, -9.54585612e-01, -1.71555114e+00,\n",
              "         -1.30500710e+00, -1.54174614e+00, -1.35448718e+00],\n",
              "        [-5.14630258e-01,  2.08300740e-01,  5.73584735e-01,\n",
              "         -1.04037297e+00, -6.63967371e-01, -9.17550325e-01],\n",
              "        [-8.31808090e-01, -2.25587392e+00, -1.62194699e-01,\n",
              "         -1.51030660e+00, -1.66142440e+00, -5.36044061e-01],\n",
              "        [ 1.07279634e+00,  1.02362275e+00,  1.39071059e+00,\n",
              "         -2.60880589e-03, -2.22636133e-01, -2.73573101e-01],\n",
              "        [-8.31257761e-01, -6.09831691e-01, -9.62893069e-01,\n",
              "         -1.83645749e+00, -1.08961654e+00, -1.11338747e+00],\n",
              "        [-1.38429475e+00, -2.05270386e+00, -2.88952994e+00,\n",
              "         -2.22947478e+00, -2.04852271e+00,  3.19950402e-01],\n",
              "        [-2.52213418e-01,  6.41656637e-01,  6.64174974e-01,\n",
              "         -1.28039956e+00, -1.70776081e+00, -1.27915657e+00],\n",
              "        [-1.82742906e+00, -3.99535418e-01, -9.40833569e-01,\n",
              "         -1.31689876e-01, -5.05158663e-01, -1.29796290e+00],\n",
              "        [-4.24307287e-01,  8.24334472e-02, -8.18800092e-01,\n",
              "         -5.57858199e-02, -1.36573935e+00, -1.12242091e+00],\n",
              "        [-7.70146906e-01, -1.18911052e+00, -9.76614833e-01,\n",
              "         -1.18702340e+00, -7.13764846e-01, -6.11830592e-01],\n",
              "        [-3.83536029e+00, -2.47117257e+00, -2.59790707e+00,\n",
              "         -1.30911517e+00, -7.13360071e-01, -1.57117820e+00],\n",
              "        [-8.82645726e-01, -9.00566876e-01, -2.23970127e+00,\n",
              "         -1.15847647e+00, -4.47150394e-02, -1.14946735e+00],\n",
              "        [-1.46883500e+00, -1.98442400e+00, -1.35622036e+00,\n",
              "         -5.91970921e-01, -1.42985094e+00, -5.58499753e-01],\n",
              "        [-6.07524633e-01, -2.28579426e+00, -1.16299832e+00,\n",
              "         -8.98666620e-01, -1.04303408e+00, -6.08866513e-01]],\n",
              "\n",
              "       [[-1.87172186e+00, -2.52870560e-01, -2.20629168e+00,\n",
              "         -2.10426331e+00, -1.94867897e+00, -2.29908681e+00],\n",
              "        [-2.21293020e+00, -4.81919003e+00, -4.28216457e+00,\n",
              "         -3.74951792e+00, -1.34270072e+00, -4.21993643e-01],\n",
              "        [ 7.79953361e-01,  2.08827019e-01,  6.92754626e-01,\n",
              "         -6.18768096e-01, -1.00963652e+00, -1.55421090e+00],\n",
              "        [-1.49296641e+00, -2.95430839e-01, -1.12684965e+00,\n",
              "         -2.24546385e+00, -2.12568355e+00, -1.73284733e+00],\n",
              "        [-1.14724112e+00, -3.41170788e+00, -3.08989787e+00,\n",
              "         -3.23242068e+00, -1.49839401e+00, -1.43550503e+00],\n",
              "        [-1.85932839e+00, -1.44557810e+00, -1.25070715e+00,\n",
              "         -1.76720667e+00, -1.21439064e+00, -1.85192990e+00],\n",
              "        [-9.39291418e-01, -1.18042922e+00,  1.04075015e-01,\n",
              "         -1.56060374e+00, -3.60909986e+00, -1.29279733e+00],\n",
              "        [-2.26899910e+00, -2.22392893e+00, -2.00182486e+00,\n",
              "         -1.40587008e+00, -6.03614390e-01, -1.52013814e+00],\n",
              "        [ 1.51018560e+00, -1.06942964e+00, -1.29883444e+00,\n",
              "         -9.69630837e-01, -1.09443402e+00, -5.09252310e-01],\n",
              "        [-1.76354504e+00, -2.44557810e+00, -2.73548818e+00,\n",
              "         -2.66199732e+00, -1.65012002e+00, -6.42965317e-01],\n",
              "        [-3.19213915e+00, -3.30823326e+00, -3.13788056e+00,\n",
              "         -1.80346262e+00, -2.94961023e+00, -1.36307585e+00],\n",
              "        [-1.46092200e+00,  1.02935767e+00,  1.57192004e+00,\n",
              "          5.65635443e-01, -1.29467356e+00,  1.58270299e-01],\n",
              "        [-1.57413065e+00, -3.32344532e+00, -1.53040552e+00,\n",
              "         -2.31421185e+00, -1.61802459e+00, -1.22624886e+00],\n",
              "        [-1.29283941e+00, -1.06724274e+00,  4.44371939e-01,\n",
              "         -1.03506541e+00, -1.41885900e+00, -1.67884398e+00],\n",
              "        [-1.47687244e+00, -1.41251636e+00, -1.71029615e+00,\n",
              "         -1.21140575e+00, -1.18648398e+00, -1.98309100e+00],\n",
              "        [-1.79642117e+00, -1.04259992e+00, -1.09843314e+00,\n",
              "         -4.62743998e-01, -1.93350244e+00, -2.34165001e+00],\n",
              "        [-7.97137499e-01, -2.35861182e+00, -1.06163538e+00,\n",
              "         -1.46063948e+00, -5.79582155e-01, -1.44926906e+00],\n",
              "        [-1.32652998e+00, -9.53222394e-01, -6.41141355e-01,\n",
              "         -2.51313239e-01, -2.06199217e+00, -1.30964243e+00],\n",
              "        [-2.22858906e+00, -2.39008904e+00, -2.43657780e+00,\n",
              "         -2.32851171e+00, -1.75280690e+00, -1.73396039e+00],\n",
              "        [-1.34526849e+00, -2.90843678e+00, -3.27434731e+00,\n",
              "         -1.18681049e+00, -1.70829308e+00, -9.90590513e-01],\n",
              "        [ 9.93292451e-01,  1.13613153e+00,  1.36469090e+00,\n",
              "          1.73996270e-01, -4.03819144e-01, -7.31577873e-01],\n",
              "        [-2.12115312e+00, -1.79566061e+00, -9.72546458e-01,\n",
              "         -7.73875177e-01, -1.54159427e+00, -1.17553806e+00],\n",
              "        [ 1.43830568e-01, -1.45326865e+00, -1.16700006e+00,\n",
              "         -7.07282007e-01, -1.49335456e+00, -2.47071886e+00],\n",
              "        [ 1.08985305e-01, -8.08156371e-01, -6.49011135e-03,\n",
              "         -3.17539155e-01, -1.05815244e+00, -5.50958693e-01],\n",
              "        [-3.20568275e+00, -5.70396709e+00, -3.47420597e+00,\n",
              "         -2.03639960e+00, -7.33172655e-01, -1.23195088e+00],\n",
              "        [-3.32112581e-01,  6.03576899e-01, -1.55185795e+00,\n",
              "          2.48272985e-01,  3.35213125e-01, -6.82544589e-01],\n",
              "        [-1.15119946e+00, -2.92771757e-01, -2.07202888e+00,\n",
              "         -1.21683681e+00, -1.04007673e+00, -2.34365702e+00],\n",
              "        [ 3.19969535e-01, -2.94271564e+00, -3.37107182e-01,\n",
              "         -1.12630272e+00, -4.66785938e-01, -9.73115563e-01],\n",
              "        [-1.83212483e+00, -5.34211826e+00, -2.48688388e+00,\n",
              "         -5.06149864e+00, -2.85508680e+00, -2.21738052e+00],\n",
              "        [ 7.48041153e-01,  9.72855806e-01,  3.33351761e-01,\n",
              "         -8.87394190e-01, -1.34560573e+00, -1.39007425e+00],\n",
              "        [-7.94022143e-01, -3.69436932e+00, -8.94728720e-01,\n",
              "         -1.92695951e+00, -1.51013196e+00, -2.09091282e+00],\n",
              "        [-1.71883032e-01,  8.06230843e-01, -1.04386115e+00,\n",
              "         -1.13701355e+00, -6.84059143e-01, -1.46125042e+00],\n",
              "        [ 4.88572657e-01,  6.33285522e-01,  2.39572167e-01,\n",
              "         -1.61708260e+00, -1.41321564e+00, -1.52364933e+00],\n",
              "        [-6.58786118e-01, -9.97020721e-01, -1.05885577e+00,\n",
              "         -1.11750507e+00, -2.16475272e+00, -1.42513871e+00],\n",
              "        [ 1.76078546e+00,  2.33923316e+00, -5.51631808e-01,\n",
              "         -1.26863718e+00, -6.80149674e-01, -7.58273184e-01],\n",
              "        [-2.93585730e+00, -2.96274185e+00, -2.37840438e+00,\n",
              "         -2.13670802e+00, -8.37542653e-01, -3.12555075e-01],\n",
              "        [-7.14261532e-01, -1.19843245e+00,  2.71467924e-01,\n",
              "         -1.71250618e+00, -8.87771070e-01, -1.23757136e+00],\n",
              "        [ 2.09437057e-01,  3.83847088e-01, -7.58024812e-01,\n",
              "         -5.93050957e-01, -1.25299835e+00,  1.26898140e-01],\n",
              "        [-2.87823772e+00, -5.08541870e+00, -3.19363546e+00,\n",
              "         -2.56205726e+00, -2.05503488e+00, -1.63143384e+00],\n",
              "        [ 1.99309421e+00,  6.85683131e-01,  4.45362777e-02,\n",
              "         -1.03878903e+00,  1.16284877e-01, -1.18362510e+00],\n",
              "        [-7.47040153e-01, -7.16845751e-01, -2.21741509e+00,\n",
              "         -1.85108161e+00, -1.23555803e+00, -5.19788861e-01],\n",
              "        [-8.90841842e-01, -1.41352320e+00,  4.94998813e-01,\n",
              "          4.85596091e-01, -2.80432791e-01, -8.14476252e-01],\n",
              "        [ 6.71508670e-01,  8.63916278e-01,  1.66779113e+00,\n",
              "          1.18184817e+00, -2.21128345e-01, -1.21997333e+00],\n",
              "        [-2.24015236e+00, -4.25147295e+00, -2.65060139e+00,\n",
              "         -1.10241604e+00,  2.12983370e-01, -1.07683420e-01],\n",
              "        [ 6.17071986e-02, -1.56396255e-01,  8.00177038e-01,\n",
              "         -2.33634114e-02,  3.36712599e-02, -1.02000642e+00],\n",
              "        [-9.77876425e-01, -1.46299815e+00, -4.89019305e-01,\n",
              "         -7.33045399e-01, -5.65943956e-01, -1.32253921e+00],\n",
              "        [ 1.78114355e-01,  1.32936537e+00,  6.70896173e-02,\n",
              "         -4.34496939e-01, -2.78367901e+00, -2.18876553e+00],\n",
              "        [-5.63129008e-01, -1.82631969e+00,  1.52211249e-01,\n",
              "         -3.30349207e-02, -7.88329661e-01, -1.28731644e+00],\n",
              "        [-3.28595424e+00, -4.68751144e+00, -2.30562282e+00,\n",
              "         -2.98699141e+00, -1.77340043e+00, -1.10064363e+00],\n",
              "        [ 7.70374119e-01, -3.82317662e-01, -2.64861166e-01,\n",
              "         -2.93185860e-01, -4.14904594e-01, -1.25739574e+00]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "w6jWfdK7dE43",
        "outputId": "1540970e-0809-4648-aee3-2408f706357c"
      },
      "source": [
        "# Visualize a bi-gram filter's outputs\n",
        "tokens = tokenizer.sequences_to_texts(X)[0].split(' ')\n",
        "sns.heatmap(conv_outputs[1], xticklabels=tokens)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c2bd75810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh_EP08yezUw"
      },
      "source": [
        "1D global max-pooling would extract the highest value from each of our `num_filters` for each `filter_size`. We could also follow this same approach to figure out which n-gram is most relevant but notice in the heatmap above that many filters don't have much variance. To mitigate this, this [paper](https://www.aclweb.org/anthology/W18-5408/) uses threshold values to determine which filters to use for interpretability. But to keep things simple, let's extract which tokens' filter outputs were extracted via max-pooling the most frequenctly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ebwto_VX-rY",
        "outputId": "5a5fc0fc-9198-4141-9195-7d8a07c3044a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['final', 'tennis', 'tournament', 'starts', 'next', 'week']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOT3TkRgexTI",
        "outputId": "ec073d8c-f94f-42e8-c9c4-96ac8334e78a"
      },
      "source": [
        "sample_index = 0\n",
        "print (f\"Origin text:\\n{text}\")\n",
        "print (f\"\\nPreprocessed text:\\n{tokenizer.sequences_to_texts(X)[0]}\")\n",
        "print (\"\\nMost important n-grams:\")\n",
        "# Process conv outputs for each unique filter size\n",
        "for i, filter_size in enumerate(FILTER_SIZES):\n",
        "\n",
        "    # Identify most important n-gram (excluding last token)\n",
        "    popular_indices = collections.Counter([np.argmax(conv_output) \\\n",
        "            for conv_output in conv_outputs[i]])\n",
        "    print (conv_outputs[0].shape)\n",
        "    #for conv_output in conv_outputs[i]:\n",
        "    #  print (conv_output.shape)\n",
        "    #print (popular_indices)\n",
        "    print (list(np.argmax(conv_outputs[i],axis=1)))\n",
        "    print ([np.argmax(conv_output) \\\n",
        "            for conv_output in conv_outputs[i]]) \n",
        "    # Get corresponding text\n",
        "    start = popular_indices.most_common(1)[-1][0]\n",
        "    print (start) \n",
        "    n_gram = \" \".join([token for token in tokens[start:start+filter_size]])\n",
        "    print (f\"[{filter_size}-gram]: {n_gram}\")"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Origin text:\n",
            "['The final tennis tournament starts next week.']\n",
            "\n",
            "Preprocessed text:\n",
            "final tennis tournament starts next week\n",
            "\n",
            "Most important n-grams:\n",
            "(50, 6)\n",
            "[4, 1, 4, 1, 2, 4, 5, 4, 1, 4, 2, 4, 2, 1, 1, 4, 1, 5, 1, 0, 1, 5, 1, 2, 1, 1, 4, 1, 1, 1, 1, 3, 4, 1, 4, 4, 1, 1, 5, 1, 3, 1, 0, 1, 5, 4, 2, 3, 3, 2]\n",
            "[4, 1, 4, 1, 2, 4, 5, 4, 1, 4, 2, 4, 2, 1, 1, 4, 1, 5, 1, 0, 1, 5, 1, 2, 1, 1, 4, 1, 1, 1, 1, 3, 4, 1, 4, 4, 1, 1, 5, 1, 3, 1, 0, 1, 5, 4, 2, 3, 3, 2]\n",
            "1\n",
            "[1-gram]: tennis\n",
            "(50, 6)\n",
            "[5, 1, 0, 0, 5, 1, 1, 5, 5, 5, 1, 4, 0, 2, 5, 4, 2, 2, 0, 5, 0, 5, 1, 1, 4, 2, 2, 2, 1, 2, 0, 0, 1, 2, 1, 5, 1, 2, 2, 2, 1, 5, 2, 3, 1, 5, 4, 4, 5, 0]\n",
            "[5, 1, 0, 0, 5, 1, 1, 5, 5, 5, 1, 4, 0, 2, 5, 4, 2, 2, 0, 5, 0, 5, 1, 1, 4, 2, 2, 2, 1, 2, 0, 0, 1, 2, 1, 5, 1, 2, 2, 2, 1, 5, 2, 3, 1, 5, 4, 4, 5, 0]\n",
            "5\n",
            "[2-gram]: week\n",
            "(50, 6)\n",
            "[1, 5, 0, 1, 0, 4, 2, 4, 0, 5, 5, 2, 5, 2, 4, 3, 4, 3, 5, 5, 2, 3, 0, 0, 4, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 5, 2, 1, 5, 0, 5, 2, 2, 4, 2, 2, 1, 2, 5, 0]\n",
            "[1, 5, 0, 1, 0, 4, 2, 4, 0, 5, 5, 2, 5, 2, 4, 3, 4, 3, 5, 5, 2, 3, 0, 0, 4, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 5, 2, 1, 5, 0, 5, 2, 2, 4, 2, 2, 1, 2, 5, 0]\n",
            "0\n",
            "[3-gram]: final tennis tournament\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-K_iL7QZCme",
        "outputId": "5ff34035-da9f-41b2-8633-90993002ecfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens[5:709]"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['week']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odv37jWze3fI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}